# imagePullSecrets is configuration to reference the k8s Secret resources the
# Helm chart's pods can get credentials from to pull their images.
imagePullSecrets:
  - regcred

# hub relates to the hub pod, responsible for running JupyterHub, its configured
# Authenticator class KubeSpawner, and its configured Proxy class
# ConfigurableHTTPProxy. KubeSpawner creates the user pods, and
# ConfigurableHTTPProxy speaks with the actual ConfigurableHTTPProxy server in
# the proxy pod.
hub:
  revisionHistoryLimit:
  config:
    JupyterHub:
      admin_access: true
      authenticator_class: generic-oauth
    GenericOAuthenticator:
      client_id: jupyterhub-shared
      client_secret: <to be defined>
      oauth_callback_url: <to be defined>
      authorize_url: <to be defined>
      token_url: <to be defined>
      userdata_url: <to be defined>
      login_service: keycloak
      username_key: preferred_username
      allow_all: true
      userdata_params:
        state: state
    Authenticator:
      admin_users:
        - <to be defined>
      admin_access: false # Disable admins to access users' notebooks
  db:
    type: sqlite-pvc
    password: <to be defined>

# scheduling relates to the user-scheduler pods and user-placeholder pods.
scheduling:
  userScheduler:
    enabled: false # See https://z2jh.jupyter.org/en/stable/administrator/optimization.html

proxy:
  service:
    type: ClusterIP

# singleuser relates to the configuration of KubeSpawner which runs in the hub
# pod, and its spawning of user pods such as jupyter-myusername.
singleuser:
  networkPolicy:
    enabled: true
    ingress: []
    egress: []
    egressAllowRules:
      cloudMetadataServer: false
      dnsPortsCloudMetadataServer: true
      dnsPortsKubeSystemNamespace: true
      dnsPortsPrivateIPs: true
      nonPrivateIPs: true
      privateIPs: false
    interNamespaceAccessLabels: ignore
    allowedIngressPorts: []
  events: true
  extraAnnotations: {}
  extraLabels:
    hub.jupyter.org/network-access-hub: "true"
  extraFiles: {}
  extraEnv:
    CHOWN_HOME: "yes"
    GRANT_SUDO: "yes"
    NOTEBOOK_ARGS: "--allow-root"
  cloudMetadata:
    # See https://github.com/jupyterhub/zero-to-jupyterhub-k8s/issues/3355
    blockWithIptables: false
  allowPrivilegeEscalation: true
  uid: 0
  fsGid: 0
  storage:
    type: dynamic
    capacity: 70Gi
  cpu:
    limit: 5
    guarantee: 2
  memory:
    limit: 32G
    guarantee: 10G
  # Add new profiles if needed. You can also reference custom images, make sur to inject the right regcred secret.
  profileList:
    - display_name: "Datascience environment"
      description: "Default environment (Python 3.12). See: https://github.com/jupyter/docker-stacks/tree/main/images/datascience-notebook."
      kubespawner_override:
        image: quay.io/jupyter/datascience-notebook:python-3.12
        image_pull_policy: IfNotPresent
        extra_resource_limits:
          nvidia.com/gpu: 2
    - display_name: "Tensorflow environment"
      description: "CUDA-based Tensorflow environment (Python 3.12, CUDA 12). See: https://github.com/jupyter/docker-stacks/tree/main/images/tensorflow-notebook."
      default: true
      kubespawner_override:
        image: quay.io/jupyter/tensorflow-notebook:cuda-python-3.12
        image_pull_policy: IfNotPresent
        extra_resource_limits:
          nvidia.com/gpu: 2
    - display_name: "Pytorch environment"
      description: "CUDA-based Pytorch environment (Python 3.12, CUDA 12). See: https://github.com/jupyter/docker-stacks/tree/main/images/pytorch-notebook."
      kubespawner_override:
        image: quay.io/jupyter/pytorch-notebook:cuda12-python-3.12
        image_pull_policy: IfNotPresent
        extra_resource_limits:
          nvidia.com/gpu: 2

ingress:
  enabled: true
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt-prod
    nginx.ingress.kubernetes.io/proxy-body-size: 2g
  ingressClassName: nginx
  hosts:
    - jupyterhub.etf-foundation.org
  pathSuffix:
  pathType: Prefix
  tls:
    - hosts:
        - jupyterhub.etf-foundation.org
      secretName: jupyterhub-tls

# cull relates to the jupyterhub-idle-culler service, responsible for evicting
# inactive singleuser pods.
#
# The configuration below, except for enabled, corresponds to command-line flags
# for jupyterhub-idle-culler as documented here:
# https://github.com/jupyterhub/jupyterhub-idle-culler#as-a-standalone-script
#
cull:
  enabled: true
  timeout: 259200 # --timeout > idle users will be disconnected after 72 hours
  every: 3600 # --cull-every > check every hour
  concurrency: 10 # --concurrency
  maxAge: 0 # --max-age

global:
  safeToShowValues: false
